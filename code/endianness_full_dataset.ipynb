{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done sh4\n",
      "done hppa\n",
      "done sparc\n",
      "done armel\n",
      "done armhf\n",
      "done s390x\n",
      "done mips64el\n",
      "done powerpc\n",
      "done riscv64\n",
      "done sparc64\n",
      "done alpha\n",
      "done arm64\n",
      "done powerpcspe\n",
      "done m68k\n",
      "done mipsel\n",
      "done i386\n",
      "done amd64\n",
      "done x32\n",
      "done mips\n",
      "done ppc64\n",
      "done s390\n",
      "done ppc64el\n",
      "done ia64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>byte_array</th>\n",
       "      <th>instruction_set</th>\n",
       "      <th>endianness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f1a7d1f5cdc5585cf36d15f44d529000</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fd52f26edf5fb6cf4f1702349d00edce</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45e05efd7b4fb113155cada98b1b76b9</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ee81f48ef9bc6d3e158b39e129373277</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7850e9fb64a382027d36203d95ca96f9</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  f1a7d1f5cdc5585cf36d15f44d529000   \n",
       "1  fd52f26edf5fb6cf4f1702349d00edce   \n",
       "2  45e05efd7b4fb113155cada98b1b76b9   \n",
       "3  ee81f48ef9bc6d3e158b39e129373277   \n",
       "4  7850e9fb64a382027d36203d95ca96f9   \n",
       "\n",
       "                                          byte_array instruction_set  \\\n",
       "0  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "1  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "2  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "3  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "4  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "\n",
       "  endianness  \n",
       "0     little  \n",
       "1     little  \n",
       "2     little  \n",
       "3     little  \n",
       "4     little  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_binary_files(directory, endiannes, instruction_set):\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if \".code\" in filename:\n",
    "            continue\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"rb\") as file:\n",
    "                byte_array = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"filename\": filename,\n",
    "                        \"byte_array\": byte_array,\n",
    "                        \"instruction_set\": instruction_set,\n",
    "                        \"endianness\": endiannes\n",
    "                    }\n",
    "                )\n",
    "        if(len(data) >= 500):\n",
    "            break\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "dfs = []\n",
    "datasetdir = \"../dataset/isa-detect/full-binaries\"\n",
    "for architecture in os.listdir(datasetdir):\n",
    "    architecture_path = os.path.join(datasetdir, architecture)\n",
    "    with open(os.path.join(architecture_path, f\"{architecture}.json\"), \"rb\") as json:\n",
    "        frame = pd.read_json(json)\n",
    "        dfs.append(read_binary_files(architecture_path, frame[\"endianness\"][1], architecture))\n",
    "    print(f\"done {architecture}\")\n",
    "# mips_df = read_binary_files(\"../dataset/mips-binaries\", label=\"mips\")\n",
    "# mipsel_df = read_binary_files(\"../dataset/mipsel-binaries\", label=\"mipsel\")\n",
    "\n",
    "data_df = pd.concat(dfs)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add byte value frequency features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_normalized_bytes(byte_array):\n",
    "#     counts = np.bincount(byte_array, minlength=256)\n",
    "#     return counts / counts.sum()\n",
    "\n",
    "\n",
    "# byte_features = data_df[\"byte_array\"].apply(count_normalized_bytes)\n",
    "# byte_columns = [f\"byte_{i}\" for i in range(256)]\n",
    "# byte_df = pd.DataFrame(byte_features.tolist(), columns=byte_columns)\n",
    "# byte_df.index = data_df.index\n",
    "# df_with_byte_features = pd.concat([data_df, byte_df], axis=1)\n",
    "\n",
    "# df_with_byte_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add heuristic features for endianness detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>byte_array</th>\n",
       "      <th>instruction_set</th>\n",
       "      <th>endianness</th>\n",
       "      <th>count_0001</th>\n",
       "      <th>count_0100</th>\n",
       "      <th>count_fffe</th>\n",
       "      <th>count_feff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f1a7d1f5cdc5585cf36d15f44d529000</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "      <td>53</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fd52f26edf5fb6cf4f1702349d00edce</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "      <td>57</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45e05efd7b4fb113155cada98b1b76b9</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "      <td>65</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ee81f48ef9bc6d3e158b39e129373277</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "      <td>38</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7850e9fb64a382027d36203d95ca96f9</td>\n",
       "      <td>[127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>sh4</td>\n",
       "      <td>little</td>\n",
       "      <td>94</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  f1a7d1f5cdc5585cf36d15f44d529000   \n",
       "1  fd52f26edf5fb6cf4f1702349d00edce   \n",
       "2  45e05efd7b4fb113155cada98b1b76b9   \n",
       "3  ee81f48ef9bc6d3e158b39e129373277   \n",
       "4  7850e9fb64a382027d36203d95ca96f9   \n",
       "\n",
       "                                          byte_array instruction_set  \\\n",
       "0  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "1  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "2  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "3  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "4  [127, 69, 76, 70, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...             sh4   \n",
       "\n",
       "  endianness  count_0001  count_0100  count_fffe  count_feff  \n",
       "0     little          53         185           0          77  \n",
       "1     little          57         104           0           3  \n",
       "2     little          65         186           0          31  \n",
       "3     little          38         123           0          35  \n",
       "4     little          94         241           0          92  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_endian_features(df):\n",
    "    def count_patterns(byte_array):\n",
    "        arr = np.array(byte_array)\n",
    "        pairs = np.column_stack((arr[:-1], arr[1:]))\n",
    "\n",
    "        count_0001 = np.sum((pairs == [0x00, 0x01]).all(axis=1))\n",
    "        count_0100 = np.sum((pairs == [0x01, 0x00]).all(axis=1))\n",
    "        count_fffe = np.sum((pairs == [0xFF, 0xFE]).all(axis=1))\n",
    "        count_feff = np.sum((pairs == [0xFE, 0xFF]).all(axis=1))\n",
    "\n",
    "        return count_0001, count_0100, count_fffe, count_feff\n",
    "\n",
    "    counts = df[\"byte_array\"].apply(count_patterns)\n",
    "\n",
    "    df[\"count_0001\"], df[\"count_0100\"], df[\"count_fffe\"], df[\"count_feff\"] = zip(\n",
    "        *counts\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_with_features = add_endian_features(data_df)\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'little'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     X_arch \u001b[38;5;241m=\u001b[39m X[arch_mask]\n\u001b[1;32m     51\u001b[0m     y_arch \u001b[38;5;241m=\u001b[39m y[arch_mask]\n\u001b[0;32m---> 53\u001b[0m     model, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evaluate_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43march\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     architecture_models[arch] \u001b[38;5;241m=\u001b[39m (model, scaler)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Function to predict endianness for a new sample\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mtrain_evaluate_architecture\u001b[0;34m(X, y, architecture_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'little'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume df_with_features has columns for instruction_set, endianness, and the count features\n",
    "\n",
    "# Endianness features\n",
    "endianness_features = ['count_0001', 'count_0100', 'count_fffe', 'count_feff']\n",
    "\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = df_with_features[endianness_features]\n",
    "y = df_with_features['endianness']\n",
    "architectures = df_with_features['instruction_set']\n",
    "\n",
    "# Function to train and evaluate a model for a single architecture\n",
    "def train_evaluate_architecture(X, y, architecture_name):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the model\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nArchitecture: {architecture_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# Dictionary to store models and scalers for each architecture\n",
    "architecture_models = {}\n",
    "\n",
    "# Train and evaluate a model for each unique architecture\n",
    "for arch in architectures.unique():\n",
    "    arch_mask = architectures == arch\n",
    "    X_arch = X[arch_mask]\n",
    "    y_arch = y[arch_mask]\n",
    "    \n",
    "    model, scaler = train_evaluate_architecture(X_arch, y_arch, arch)\n",
    "    architecture_models[arch] = (model, scaler)\n",
    "\n",
    "# Function to predict endianness for a new sample\n",
    "def predict_endianness(sample, architecture):\n",
    "    if architecture not in architecture_models:\n",
    "        raise ValueError(f\"No model available for architecture: {architecture}\")\n",
    "    \n",
    "    model, scaler = architecture_models[architecture]\n",
    "    sample_scaled = scaler.transform([sample])\n",
    "    prediction = model.predict(sample_scaled)\n",
    "    probability = model.predict_proba(sample_scaled)\n",
    "    \n",
    "    return prediction[0], probability[0]\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample predictions:\")\n",
    "for arch in architectures.unique():\n",
    "    sample = X[architectures == arch].iloc[0]  # Get first sample of each architecture\n",
    "    prediction, probability = predict_endianness(sample, arch)\n",
    "    print(f\"Architecture: {arch}\")\n",
    "    print(f\"Predicted endianness: {prediction}\")\n",
    "    print(f\"Probabilities: {probability}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
