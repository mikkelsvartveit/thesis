{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train/Test Split Model Comparison\n",
        "\n",
        "This notebook trains six different models on a standard 80/20 train/test split of the ISAdetect dataset for the 'endianness' target feature and compares their final test accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm  # Use notebook version of tqdm\n",
        "from datetime import datetime\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to sys.path to import project modules\n",
        "# Assumes the notebook is run from the 'analysis' directory\n",
        "module_path = os.path.abspath(os.path.join(\"..\"))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "from dataset_loaders import get_dataset\n",
        "from models import get_model\n",
        "from transforms import get_transform\n",
        "from validators.train_test_utils import set_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom collate function to handle the dataset output\n",
        "# Pads image tensors to the maximum size in the batch\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Find the maximum size of tensors in the batch\n",
        "    max_size = max([item[0].size(0) for item in batch])\n",
        "\n",
        "    # Pad tensors to the maximum size\n",
        "    padded_images = []\n",
        "    for item in batch:\n",
        "        tensor = item[0]\n",
        "        padding_size = max_size - tensor.size(0)\n",
        "        # Pad with zeros at the end\n",
        "        padded_tensor = torch.nn.functional.pad(tensor, (0, padding_size))\n",
        "        padded_images.append(padded_tensor)\n",
        "\n",
        "    images = torch.stack(padded_images)\n",
        "    labels = [item[1] for item in batch] # Keep labels as a list of dicts\n",
        "    # file_paths = [item[2] for item in batch] # If file_path is needed later as a batch\n",
        "\n",
        "    return images, labels # Return images and the list of label dicts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "TARGET_FEATURE = \"endianness\"\n",
        "DATASET_NAME = \"ISAdetectDataset\"  # Or choose another like 'CpuRecDataset'\n",
        "DATASET_BASE_PATH = Path(\n",
        "    os.environ.get(\"DATASET_BASE_PATH\", \"../../dataset\")\n",
        ")  # Adjust if needed\n",
        "MODEL_NAMES = [\n",
        "    \"Simple1d\",\n",
        "    \"Simple1dEmbedding\",\n",
        "    \"Simple2d\",\n",
        "    \"Simple2dEmbedding\",\n",
        "    \"ResNet50\",\n",
        "    \"ResNet50Embedding\",\n",
        "]\n",
        "TRAIN_SPLIT_RATIO = 0.8\n",
        "SEED = 42\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.0001\n",
        "WEIGHT_DECAY = 0.01\n",
        "OPTIMIZER = \"AdamW\"\n",
        "CRITERION = \"CrossEntropyLoss\"\n",
        "DEVICE = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Target Feature: {TARGET_FEATURE}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Models: {MODEL_NAMES}\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset and Prepare Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load Data ---\n",
        "# Using default transforms for now, adjust if needed\n",
        "transforms = None  # Ensure default transforms are used\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = get_dataset(\n",
        "    name=DATASET_NAME,\n",
        "    transform=transforms,\n",
        "    dataset_base_path=DATASET_BASE_PATH,\n",
        "    target_feature=TARGET_FEATURE,\n",
        "    params={\n",
        "        # This path should point to the directory containing architecture subfolders (arm, mips, etc.)\n",
        "        \"dataset_path\": \"ISAdetect/ISAdetect_full_dataset\",\n",
        "        \"feature_csv_path\": \"ISAdetect-features.csv\",  # Relative path within DATASET_BASE_PATH\n",
        "    },\n",
        ")\n",
        "\n",
        "# Prepare for stratified split\n",
        "targets = [item[TARGET_FEATURE] for item in full_dataset.metadata]\n",
        "indices = list(range(len(full_dataset)))\n",
        "\n",
        "# Perform stratified train/test split\n",
        "train_idx, test_idx, _, _ = train_test_split(\n",
        "    indices,\n",
        "    targets,\n",
        "    stratify=targets,\n",
        "    test_size=1.0 - TRAIN_SPLIT_RATIO,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "test_dataset = Subset(full_dataset, test_idx)\n",
        "\n",
        "print(f\"Full dataset size: {len(full_dataset)}\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Fit LabelEncoder on training data labels\n",
        "label_encoder = LabelEncoder()\n",
        "# Need to get the actual labels from the subset indices\n",
        "train_labels = [full_dataset.metadata[i][TARGET_FEATURE] for i in train_idx]\n",
        "label_encoder.fit(train_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # Adjust based on your system\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,  # Adjust based on your system\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Evaluation Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Training & Evaluation ---\n",
        "model_results = {}\n",
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "    print(f\"\\n{'='*10} Training Model: {model_name} {'='*10}\")\n",
        "    set_seed(SEED)  # Reset seed for each model for consistent initialization\n",
        "\n",
        "    # Get model class\n",
        "    # Assuming get_model can infer params or uses defaults\n",
        "    # Need to pass num_classes based on the dataset\n",
        "    model_class = get_model(name=model_name, params={\"num_classes\": num_classes})\n",
        "    model = model_class(num_classes=num_classes)  # Instantiate\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Criterion and Optimizer\n",
        "    criterion = getattr(nn, CRITERION)()\n",
        "    optimizer = getattr(torch.optim, OPTIMIZER)(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
        "        for batch_idx, (images, labels_list) in enumerate(progress_bar):\n",
        "            images = images.to(DEVICE)\n",
        "            # Extract the target feature string label for encoding from the list of labels\n",
        "            str_labels = [label_item[TARGET_FEATURE] for label_item in labels_list]\n",
        "            encoded_labels = torch.from_numpy(label_encoder.transform(str_labels)).to(\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, encoded_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation Loop\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    file_predictions_map = {}\n",
        "    file_true_labels_map = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
        "        for images, labels_list in progress_bar:\n",
        "            images = images.to(DEVICE)\n",
        "            # Extract file paths and target feature labels from the list of labels\n",
        "            file_paths = [label_item[\"file_path\"] for label_item in labels_list]\n",
        "            str_labels = [label_item[TARGET_FEATURE] for label_item in labels_list]\n",
        "            encoded_labels = torch.from_numpy(label_encoder.transform(str_labels)).to(\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, encoded_labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            batch_predictions = predicted.cpu().numpy()\n",
        "            batch_true_labels = encoded_labels.cpu().numpy()\n",
        "\n",
        "            # Store predictions by parent file for majority voting (like in logo_cv)\n",
        "            for pred, true_label, file_path in zip(\n",
        "                batch_predictions, batch_true_labels, file_paths\n",
        "            ):\n",
        "                # Use file path as key; might need adjustment if paths aren't unique identifiers\n",
        "                parent_file = os.path.basename(\n",
        "                    file_path\n",
        "                )  # Or some other way to group chunks\n",
        "                if parent_file not in file_predictions_map:\n",
        "                    file_predictions_map[parent_file] = []\n",
        "                    file_true_labels_map[parent_file] = true_label\n",
        "                file_predictions_map[parent_file].append(pred)\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "    # Calculate majority voting accuracy\n",
        "    file_level_predictions = []\n",
        "    file_level_true_labels = []\n",
        "    for file_key in file_predictions_map:\n",
        "        chunk_preds = file_predictions_map[file_key]\n",
        "        # Check if chunk_preds is not empty before bincount\n",
        "        if chunk_preds:\n",
        "            vote_distribution = np.bincount(chunk_preds, minlength=num_classes)\n",
        "            file_prediction = vote_distribution.argmax()\n",
        "            file_true_label = file_true_labels_map[file_key]\n",
        "            file_level_predictions.append(file_prediction)\n",
        "            file_level_true_labels.append(file_true_label)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Warning: No predictions found for file key {file_key}\"\n",
        "            )  # Handle cases with no predictions\n",
        "\n",
        "    # Ensure there are predictions to calculate accuracy\n",
        "    if file_level_predictions:\n",
        "        file_level_accuracy = np.mean(\n",
        "            np.array(file_level_predictions) == np.array(file_level_true_labels)\n",
        "        )\n",
        "    else:\n",
        "        file_level_accuracy = 0.0  # Or handle as NaN or error\n",
        "        print(\"Warning: No file-level predictions were made, accuracy set to 0.\")\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  Average Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"  File-level Test Accuracy: {100 * file_level_accuracy:.2f}%\")\n",
        "\n",
        "    model_results[model_name] = file_level_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plotting ---\n",
        "model_names_sorted = sorted(model_results.keys())\n",
        "accuracies_sorted = [model_results[m] for m in model_names_sorted]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(model_names_sorted, accuracies_sorted, color=\"skyblue\")\n",
        "\n",
        "# Add accuracy values on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2.0,\n",
        "        yval,\n",
        "        f\"{yval:.3f}\",\n",
        "        va=\"bottom\",\n",
        "        ha=\"center\",\n",
        "    )  # Adjust position\n",
        "\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Test Accuracy (File-Level)\")\n",
        "plt.title(f\"Model Comparison on '{TARGET_FEATURE}' (Train/Test Split)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylim(0, 1.05)  # Extend y-limit slightly for text visibility\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
