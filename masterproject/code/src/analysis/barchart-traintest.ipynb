{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train/Test Split Model Comparison\n",
        "\n",
        "This notebook trains six different models on a standard 80/20 train/test split of the ISAdetect dataset for the 'endianness' target feature and compares their final test accuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm  # Use notebook version of tqdm\n",
        "from datetime import datetime\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to sys.path to import project modules\n",
        "# Assumes the notebook is run from the 'analysis' directory\n",
        "module_path = os.path.abspath(os.path.join(\"..\"))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "from dataset_loaders import get_dataset\n",
        "from models import get_model\n",
        "from transforms import get_transform\n",
        "from validators.train_test_utils import set_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Target Feature: endianness\n",
            "Dataset: ISAdetectDataset\n",
            "Models: ['Simple1d', 'Simple1dEmbedding', 'Simple2d', 'Simple2dEmbedding', 'ResNet50', 'ResNet50Embedding']\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "TARGET_FEATURE = \"endianness\"\n",
        "DATASET_NAME = \"ISAdetectDataset\"  # Or choose another like 'CpuRecDataset'\n",
        "DATASET_BASE_PATH = Path(\n",
        "    os.environ.get(\"DATASET_BASE_PATH\", \"../dataset\")\n",
        ")  # Adjust if needed\n",
        "MODEL_NAMES = [\n",
        "    \"Simple1d\",\n",
        "    \"Simple1dEmbedding\",\n",
        "    \"Simple2d\",\n",
        "    \"Simple2dEmbedding\",\n",
        "    \"ResNet50\",\n",
        "    \"ResNet50Embedding\",\n",
        "]\n",
        "TRAIN_SPLIT_RATIO = 0.8\n",
        "SEED = 42\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "OPTIMIZER = \"AdamW\"\n",
        "CRITERION = \"CrossEntropyLoss\"\n",
        "DEVICE = torch.device(\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Target Feature: {TARGET_FEATURE}\")\n",
        "print(f\"Dataset: {DATASET_NAME}\")\n",
        "print(f\"Models: {MODEL_NAMES}\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset and Prepare Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../dataset/ISAdetect_full_dataset'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Ensure default transforms are used\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the full dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_base_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_BASE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_FEATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This path should point to the directory containing architecture subfolders (arm, mips, etc.)\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISAdetect_full_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_csv_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISAdetect-features.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Relative path within DATASET_BASE_PATH\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Prepare for stratified split\u001b[39;00m\n\u001b[1;32m     19\u001b[0m targets \u001b[38;5;241m=\u001b[39m [item[TARGET_FEATURE] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m full_dataset\u001b[38;5;241m.\u001b[39mmetadata]\n",
            "File \u001b[0;32m~/Developer/ntnu/thesis/masterproject/code/src/dataset_loaders/__init__.py:25\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(transform, dataset_base_path, target_feature, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MipsMipselDataset(transform\u001b[38;5;241m=\u001b[39mtransform, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISAdetectDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mISAdetectDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpuRecDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CpuRecDataset(\n\u001b[1;32m     32\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     33\u001b[0m         target_feature\u001b[38;5;241m=\u001b[39mtarget_feature,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m     35\u001b[0m     )\n",
            "File \u001b[0;32m~/Developer/ntnu/thesis/masterproject/code/src/dataset_loaders/ISAdetectDataset.py:30\u001b[0m, in \u001b[0;36mISAdetectDataset.__init__\u001b[0;34m(self, dataset_path, feature_csv_path, target_feature, transform, per_architecture_limit, file_byte_read_limit, use_code_only, max_file_splits)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Collect files\u001b[39;00m\n\u001b[1;32m     29\u001b[0m metadata_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 30\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/pathlib.py:1056\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_child_relpath(name)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/ISAdetect_full_dataset'"
          ]
        }
      ],
      "source": [
        "# --- Load Data ---\n",
        "# Using default transforms for now, adjust if needed\n",
        "transforms = None  # Ensure default transforms are used\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = get_dataset(\n",
        "    name=DATASET_NAME,\n",
        "    transform=transforms,\n",
        "    dataset_base_path=DATASET_BASE_PATH,\n",
        "    target_feature=TARGET_FEATURE,\n",
        "    params={\n",
        "        # This path should point to the directory containing architecture subfolders (arm, mips, etc.)\n",
        "        \"dataset_path\": \"ISAdetect_full_dataset\",\n",
        "        \"feature_csv_path\": \"ISAdetect-features.csv\",  # Relative path within DATASET_BASE_PATH\n",
        "    },\n",
        ")\n",
        "\n",
        "# Prepare for stratified split\n",
        "targets = [item[TARGET_FEATURE] for item in full_dataset.metadata]\n",
        "indices = list(range(len(full_dataset)))\n",
        "\n",
        "# Perform stratified train/test split\n",
        "train_idx, test_idx, _, _ = train_test_split(\n",
        "    indices,\n",
        "    targets,\n",
        "    stratify=targets,\n",
        "    test_size=1.0 - TRAIN_SPLIT_RATIO,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "test_dataset = Subset(full_dataset, test_idx)\n",
        "\n",
        "print(f\"Full dataset size: {len(full_dataset)}\")\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Fit LabelEncoder on training data labels\n",
        "label_encoder = LabelEncoder()\n",
        "# Need to get the actual labels from the subset indices\n",
        "train_labels = [full_dataset.metadata[i][TARGET_FEATURE] for i in train_idx]\n",
        "label_encoder.fit(train_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # Adjust based on your system\n",
        "    pin_memory=True,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,  # Adjust based on your system\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Evaluation Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Training & Evaluation ---\n",
        "model_results = {}\n",
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "    print(f\"\\n{'='*10} Training Model: {model_name} {'='*10}\")\n",
        "    set_seed(SEED)  # Reset seed for each model for consistent initialization\n",
        "\n",
        "    # Get model class\n",
        "    # Assuming get_model can infer params or uses defaults\n",
        "    # Need to pass num_classes based on the dataset\n",
        "    model_class = get_model(name=model_name, params={\"num_classes\": num_classes})\n",
        "    model = model_class(num_classes=num_classes)  # Instantiate\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # Criterion and Optimizer\n",
        "    criterion = getattr(nn, CRITERION)()\n",
        "    optimizer = getattr(torch.optim, OPTIMIZER)(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
        "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
        "            images = images.to(DEVICE)\n",
        "            # Extract the target feature string label for encoding\n",
        "            str_labels = labels[TARGET_FEATURE]\n",
        "            encoded_labels = torch.from_numpy(label_encoder.transform(str_labels)).to(\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, encoded_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation Loop\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "    file_predictions_map = {}\n",
        "    file_true_labels_map = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(test_loader, desc=\"Evaluating\")\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(DEVICE)\n",
        "            file_paths = labels[\"file_path\"]  # Assuming 'file_path' is in metadata\n",
        "            str_labels = labels[TARGET_FEATURE]\n",
        "            encoded_labels = torch.from_numpy(label_encoder.transform(str_labels)).to(\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, encoded_labels)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            batch_predictions = predicted.cpu().numpy()\n",
        "            batch_true_labels = encoded_labels.cpu().numpy()\n",
        "\n",
        "            # Store predictions by parent file for majority voting (like in logo_cv)\n",
        "            for pred, true_label, file_path in zip(\n",
        "                batch_predictions, batch_true_labels, file_paths\n",
        "            ):\n",
        "                # Use file path as key; might need adjustment if paths aren't unique identifiers\n",
        "                parent_file = os.path.basename(\n",
        "                    file_path\n",
        "                )  # Or some other way to group chunks\n",
        "                if parent_file not in file_predictions_map:\n",
        "                    file_predictions_map[parent_file] = []\n",
        "                    file_true_labels_map[parent_file] = true_label\n",
        "                file_predictions_map[parent_file].append(pred)\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "    # Calculate majority voting accuracy\n",
        "    file_level_predictions = []\n",
        "    file_level_true_labels = []\n",
        "    for file_key in file_predictions_map:\n",
        "        chunk_preds = file_predictions_map[file_key]\n",
        "        # Check if chunk_preds is not empty before bincount\n",
        "        if chunk_preds:\n",
        "            vote_distribution = np.bincount(chunk_preds, minlength=num_classes)\n",
        "            file_prediction = vote_distribution.argmax()\n",
        "            file_true_label = file_true_labels_map[file_key]\n",
        "            file_level_predictions.append(file_prediction)\n",
        "            file_level_true_labels.append(file_true_label)\n",
        "        else:\n",
        "            print(\n",
        "                f\"Warning: No predictions found for file key {file_key}\"\n",
        "            )  # Handle cases with no predictions\n",
        "\n",
        "    # Ensure there are predictions to calculate accuracy\n",
        "    if file_level_predictions:\n",
        "        file_level_accuracy = np.mean(\n",
        "            np.array(file_level_predictions) == np.array(file_level_true_labels)\n",
        "        )\n",
        "    else:\n",
        "        file_level_accuracy = 0.0  # Or handle as NaN or error\n",
        "        print(\"Warning: No file-level predictions were made, accuracy set to 0.\")\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  Average Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"  File-level Test Accuracy: {100 * file_level_accuracy:.2f}%\")\n",
        "\n",
        "    model_results[model_name] = file_level_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Plotting ---\n",
        "model_names_sorted = sorted(model_results.keys())\n",
        "accuracies_sorted = [model_results[m] for m in model_names_sorted]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(model_names_sorted, accuracies_sorted, color=\"skyblue\")\n",
        "\n",
        "# Add accuracy values on top of bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2.0,\n",
        "        yval,\n",
        "        f\"{yval:.3f}\",\n",
        "        va=\"bottom\",\n",
        "        ha=\"center\",\n",
        "    )  # Adjust position\n",
        "\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Test Accuracy (File-Level)\")\n",
        "plt.title(f\"Model Comparison on '{TARGET_FEATURE}' (Train/Test Split)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylim(0, 1.05)  # Extend y-limit slightly for text visibility\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
